# app/scripts/stat_leaders_phone_v2.py
#
# Phone-sized stat leader posters (1440x2560), generated by scraping ESPN HTML tables
# (so player names + team abbreviations are present).
#
# Generates 10 posters:
#  1) Passing Yards
#  2) Passing TDs
#  3) Interceptions Thrown
#  4) Rushing Yards
#  5) Rushing TDs
#  6) Receiving Yards
#  7) Receiving TDs
#  8) Sacks
#  9) Tackles (Total)
# 10) Interceptions (Defense)
#
# Upload + cache prefix:
#   stat_leaders_v2/{season}/{regular|playoffs}/

import os
import re
import time
import tempfile
from typing import Dict, Any, List, Tuple, Optional

import requests
from PIL import Image, ImageDraw, ImageFont

from app.services.storage_supabase import upload_file_return_url, cached_urls_for_prefix


# =========================
# Config
# =========================

W, H = 1440, 2560  # phone-crisp
MARGIN_X = 90
TOP_Y = 140

ROW_H = 175
HEADER_H = 240

ESPN_BASE = "https://www.espn.com"

# ESPN pages are HTML; these URLs generally contain server-rendered tables.
# NOTE: ESPN occasionally changes param names. This script is defensive and
# will raise a clear error if it can't find/parse tables.
def _url_offense(season: int, seasontype: int, table: str, sort: str) -> str:
    return (
        f"{ESPN_BASE}/nfl/stats/player/_/view/offense/"
        f"season/{season}/seasontype/{seasontype}/table/{table}/sort/{sort}/dir/desc"
    )

def _url_defense(season: int, seasontype: int, table: str, sort: str) -> str:
    return (
        f"{ESPN_BASE}/nfl/stats/player/_/view/defense/"
        f"season/{season}/seasontype/{seasontype}/table/{table}/sort/{sort}/dir/desc"
    )


# Your 10 categories (no passes defended; interceptions thrown is back)
CATEGORIES = [
    # OFFENSE - PASSING
    {
        "key": "passing_yards",
        "title": "Passing Yards",
        "url_fn": lambda s, t: _url_offense(s, t, "passing", "passingYards"),
        "stat_labels": ["YDS", "YARDS"],  # column header match
    },
    {
        "key": "passing_tds",
        "title": "Passing TDs",
        "url_fn": lambda s, t: _url_offense(s, t, "passing", "passingTouchdowns"),
        "stat_labels": ["TD", "TDS"],
    },
    {
        "key": "interceptions_thrown",
        "title": "Interceptions Thrown",
        "url_fn": lambda s, t: _url_offense(s, t, "passing", "interceptions"),
        "stat_labels": ["INT"],
    },

    # OFFENSE - RUSHING
    {
        "key": "rushing_yards",
        "title": "Rushing Yards",
        "url_fn": lambda s, t: _url_offense(s, t, "rushing", "rushingYards"),
        "stat_labels": ["YDS", "YARDS"],
    },
    {
        "key": "rushing_tds",
        "title": "Rushing TDs",
        "url_fn": lambda s, t: _url_offense(s, t, "rushing", "rushingTouchdowns"),
        "stat_labels": ["TD", "TDS"],
    },

    # OFFENSE - RECEIVING
    {
        "key": "receiving_yards",
        "title": "Receiving Yards",
        "url_fn": lambda s, t: _url_offense(s, t, "receiving", "receivingYards"),
        "stat_labels": ["YDS", "YARDS"],
    },
    {
        "key": "receiving_tds",
        "title": "Receiving TDs",
        "url_fn": lambda s, t: _url_offense(s, t, "receiving", "receivingTouchdowns"),
        "stat_labels": ["TD", "TDS"],
    },

    # DEFENSE
    {
        "key": "sacks",
        "title": "Sacks",
        "url_fn": lambda s, t: _url_defense(s, t, "defensive", "sacks"),
        "stat_labels": ["SACK", "SACKS"],
    },
    {
        "key": "tackles",
        "title": "Tackles",
        "url_fn": lambda s, t: _url_defense(s, t, "defensive", "totalTackles"),
        "stat_labels": ["TOT", "TOTAL", "TCK", "TACKLES"],
    },
    {
        "key": "interceptions_def",
        "title": "Interceptions",
        "url_fn": lambda s, t: _url_defense(s, t, "defensive", "interceptions"),
        "stat_labels": ["INT"],
    },
]


# =========================
# Fonts
# =========================

def _load_font(size: int, bold: bool = False) -> ImageFont.FreeTypeFont:
    candidates = []
    if bold:
        candidates = [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
        ]
    else:
        candidates = [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
        ]

    for p in candidates:
        if os.path.exists(p):
            return ImageFont.truetype(p, size=size)

    return ImageFont.load_default()


FONT_TITLE = _load_font(74, bold=True)
FONT_SUB = _load_font(42, bold=False)
FONT_ROW_NAME = _load_font(52, bold=True)
FONT_ROW_TEAM = _load_font(42, bold=False)
FONT_ROW_VAL = _load_font(64, bold=True)
FONT_RANK = _load_font(46, bold=True)


# =========================
# Helpers
# =========================

def _kind(seasontype: int) -> str:
    return "regular" if int(seasontype) == 2 else "playoffs"


def _prefix(season: int, seasontype: int) -> str:
    return f"stat_leaders_v2/{season}/{_kind(seasontype)}/"


def _safe_filename(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    return s.strip("_") or "category"


def _http_get_html(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; P5TechStatLeaders/1.0)",
        "Accept": "text/html,application/xhtml+xml",
    }
    r = requests.get(url, headers=headers, timeout=30)
    r.raise_for_status()
    return r.text


def _strip(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def _extract_tables_from_html(html: str) -> List[Dict[str, Any]]:
    """
    ESPN tables are usually:
      <table> ... <thead> <tr><th>...</th>...</tr> ... <tbody> <tr><td>...</td>...</tr> ...
    We parse with regex + simple heuristics to avoid extra dependencies.
    Returns list of dicts: {headers: [...], rows: [[...], ...]}
    """
    # very lightweight HTML parsing using regex blocks for <table>...</table>
    tables = []
    for table_match in re.finditer(r"<table[^>]*>.*?</table>", html, flags=re.DOTALL | re.IGNORECASE):
        table_html = table_match.group(0)

        # headers
        thead = re.search(r"<thead[^>]*>.*?</thead>", table_html, flags=re.DOTALL | re.IGNORECASE)
        if not thead:
            continue

        ths = re.findall(r"<th[^>]*>(.*?)</th>", thead.group(0), flags=re.DOTALL | re.IGNORECASE)
        headers = [_strip(re.sub(r"<.*?>", "", h)) for h in ths]
        headers = [h for h in headers if h]

        # body rows
        tbody = re.search(r"<tbody[^>]*>.*?</tbody>", table_html, flags=re.DOTALL | re.IGNORECASE)
        if not tbody:
            continue

        row_htmls = re.findall(r"<tr[^>]*>.*?</tr>", tbody.group(0), flags=re.DOTALL | re.IGNORECASE)
        rows: List[List[str]] = []
        for rh in row_htmls:
            tds = re.findall(r"<td[^>]*>(.*?)</td>", rh, flags=re.DOTALL | re.IGNORECASE)
            if not tds:
                continue
            cells = [_strip(re.sub(r"<.*?>", "", td)) for td in tds]
            cells = [c for c in cells]  # keep empties if present; but normalized
            rows.append(cells)

        # require minimum columns and rows
        if headers and rows and len(headers) >= 3 and len(rows) >= 5:
            tables.append({"headers": headers, "rows": rows})

    return tables


def _pick_best_table(tables: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """
    ESPN pages sometimes contain multiple tables (filters, etc).
    The stats table typically has many rows and a header containing 'PLAYER' and 'TEAM'.
    """
    best = None
    best_score = -1
    for t in tables:
        headers = [h.upper() for h in t["headers"]]
        score = 0
        if any("PLAYER" in h for h in headers):
            score += 5
        if any("TEAM" in h for h in headers):
            score += 3
        score += min(len(t["rows"]), 50) / 10  # more rows => better
        score += min(len(t["headers"]), 20) / 10
        if score > best_score:
            best_score = score
            best = t
    return best


def _find_stat_col(headers: List[str], desired_labels: List[str]) -> int:
    """
    Find the index of the stat column in the table based on header names.
    """
    upper = [h.upper() for h in headers]
    desired = [d.upper() for d in desired_labels]

    # direct match
    for i, h in enumerate(upper):
        if h in desired:
            return i

    # contains match (e.g. "TOTAL TACKLES" contains "TOTAL")
    for i, h in enumerate(upper):
        for d in desired:
            if d in h:
                return i

    raise ValueError(f"Could not find stat column for labels={desired_labels} in headers={headers}")


def _find_player_team_cols(headers: List[str]) -> Tuple[int, int]:
    upper = [h.upper() for h in headers]

    player_i = None
    team_i = None

    for i, h in enumerate(upper):
        if "PLAYER" in h and player_i is None:
            player_i = i
        if "TEAM" in h and team_i is None:
            team_i = i

    # ESPN sometimes uses "POS" etc; still, PLAYER is almost always present
    if player_i is None:
        player_i = 0

    if team_i is None:
        # Sometimes the team is embedded in the player cell. We'll handle later.
        team_i = -1

    return player_i, team_i


def _split_player_team_if_needed(player_cell: str) -> Tuple[str, str]:
    """
    Some ESPN tables show "Josh Allen BUF" in a single cell.
    Try to split a trailing team abbrev.
    """
    s = _strip(player_cell)
    m = re.match(r"^(.*)\s([A-Z]{2,4})$", s)
    if m:
        return _strip(m.group(1)), m.group(2)
    return s, "--"


def _scrape_top10(season: int, seasontype: int, url: str, stat_labels: List[str]) -> List[Tuple[str, str, str]]:
    html = _http_get_html(url)
    tables = _extract_tables_from_html(html)
    table = _pick_best_table(tables)

    if not table:
        raise RuntimeError(f"Could not find a stats table on ESPN page: {url}")

    headers = table["headers"]
    rows = table["rows"]

    player_i, team_i = _find_player_team_cols(headers)
    stat_i = _find_stat_col(headers, stat_labels)

    out: List[Tuple[str, str, str]] = []
    for r in rows:
        if len(r) <= max(player_i, stat_i):
            continue

        raw_player = r[player_i] if player_i >= 0 else ""
        raw_team = r[team_i] if (team_i >= 0 and team_i < len(r)) else ""

        name = _strip(raw_player)
        team = _strip(raw_team)
        val = _strip(r[stat_i])

        # If team column missing or empty, try to split from player cell
        if not team or team == "--":
            name, team_guess = _split_player_team_if_needed(name)
            if team_guess != "--":
                team = team_guess
        if not team:
            team = "--"

        if not name:
            continue
        if not val:
            val = "--"

        out.append((name, team, val))
        if len(out) >= 10:
            break

    if len(out) < 10:
        # ESPN sometimes hides rows behind pagination; but most leader sorts show plenty.
        # Still, fail loudly so you notice quickly.
        raise RuntimeError(f"Only scraped {len(out)} rows (expected 10) from: {url}")

    return out


# =========================
# Drawing (KEEP DESIGN SAME)
# =========================

def _draw_bg(im: Image.Image) -> None:
    px = im.load()
    for y in range(H):
        t = y / (H - 1)
        base = int(12 + 28 * t)  # 12..40
        for x in range(W):
            px[x, y] = (base, base, base)


def _draw_header(draw: ImageDraw.ImageDraw, title: str, subtitle: str) -> None:
    draw.text((MARGIN_X, TOP_Y), title, font=FONT_TITLE, fill=(255, 255, 255))
    draw.text((MARGIN_X, TOP_Y + 92), subtitle, font=FONT_SUB, fill=(190, 190, 190))
    y = TOP_Y + HEADER_H - 35
    draw.line((MARGIN_X, y, W - MARGIN_X, y), fill=(85, 85, 85), width=3)


def _draw_rows(draw: ImageDraw.ImageDraw, rows: List[Tuple[str, str, str]]) -> None:
    start_y = TOP_Y + HEADER_H

    for i, (name, team, val) in enumerate(rows, start=1):
        y = start_y + (i - 1) * ROW_H

        draw.text((MARGIN_X, y + 52), f"{i}", font=FONT_RANK, fill=(170, 170, 170))

        name_x = MARGIN_X + 90
        draw.text((name_x, y + 32), name, font=FONT_ROW_NAME, fill=(255, 255, 255))
        draw.text((name_x, y + 98), team, font=FONT_ROW_TEAM, fill=(170, 170, 170))

        val_w = draw.textlength(val, font=FONT_ROW_VAL)
        draw.text((W - MARGIN_X - val_w, y + 45), val, font=FONT_ROW_VAL, fill=(255, 255, 255))

        draw.line(
            (MARGIN_X, y + ROW_H - 18, W - MARGIN_X, y + ROW_H - 18),
            fill=(55, 55, 55),
            width=2,
        )


def _make_one_poster(
    season: int,
    seasontype: int,
    category_title: str,
    filename_slug: str,
    rows: List[Tuple[str, str, str]],
) -> str:
    im = Image.new("RGB", (W, H), (10, 10, 10))
    _draw_bg(im)
    draw = ImageDraw.Draw(im)

    st = "Regular Season" if int(seasontype) == 2 else "Postseason"
    subtitle = f"{season} • {st} • Top 10"

    _draw_header(draw, category_title, subtitle)
    _draw_rows(draw, rows)

    out_dir = tempfile.mkdtemp(prefix=f"stat_leaders_{season}_{seasontype}_")
    out_path = os.path.join(out_dir, f"{filename_slug}.png")
    im.save(out_path, format="PNG", optimize=True)
    return out_path


# =========================
# Public API
# =========================

def generate_stat_leaders_and_upload(season: int, seasontype: int) -> List[str]:
    """
    Returns list of public URLs for the 10 posters.
    Uses Supabase cache prefix stat_leaders_v2/{season}/{regular|playoffs}/
    """
    prefix = _prefix(season, seasontype)

    cached = cached_urls_for_prefix(prefix)
    if cached:
        return cached

    uploaded_urls: List[str] = []
    errors: List[str] = []

    for c in CATEGORIES:
        title = c["title"]
        url = c["url_fn"](season, seasontype)
        labels = c["stat_labels"]
        slug = _safe_filename(title)

        try:
            rows = _scrape_top10(season, seasontype, url, labels)
            png_path = _make_one_poster(season, seasontype, title, slug, rows)
            key = f"{prefix}{os.path.basename(png_path)}"
            uploaded_urls.append(upload_file_return_url(png_path, key))
        except Exception as e:
            errors.append(f"{title}: {e}")

    if not uploaded_urls:
        raise RuntimeError("No stat leader posters were generated.\n" + "\n".join(errors))

    # Stable ordering by your category order (not by URL)
    return uploaded_urls


# Allow local/GHA runs
if __name__ == "__main__":
    season = int(os.getenv("SEASON", "2025"))
    seasontype = int(os.getenv("SEASONTYPE", "2"))
    urls = generate_stat_leaders_and_upload(season, seasontype)
    print(f"Uploaded {len(urls)} posters:")
    for u in urls:
        print(u)
